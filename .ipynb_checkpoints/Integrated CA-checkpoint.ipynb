{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c25c43",
   "metadata": {},
   "source": [
    "# Integrated CA for Machine Learning and Data Visualization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed85e19",
   "metadata": {},
   "source": [
    "## Student Name: Mijail Fausto Blanco Vargas\n",
    "## Student Number: 2023012\n",
    "## Git Hub Link:  https://github.com/mijailbv/Integrated_CA_ML_-DVis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134bebe",
   "metadata": {},
   "source": [
    "## Assessment details\n",
    "### 1) Discuss and explain the purpose of a recommendation system for online retail business in machine learning. Briefly compare Content and Collaborative filtering using any dataset of your choice (Datasets used in the class tutorials or exercises are not allowed to use in this CA2). Train and test machine learning models for the user-user or item-item collaborative filtering. Justify your recommendations for the considered scenario by providing a conceptual insight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c3528",
   "metadata": {},
   "source": [
    "## 1.1) Discussion and Explanation of a Recommendation system in Machine Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4482c",
   "metadata": {},
   "source": [
    "A recomendation system in online retail has the purpose to suggest customers or users what services or products they would be interested to purchase, adquire, or buy according to previous data like previous sells, categories of the products, streaming services, demographics, age, among others are analyzed creating the recommendation system using Machine Learning Models that predicts what are the interests of the customers and give us the option to offer it to them. For example, when you buy clothes in Shein, after you buy them, you got new recommendation clothers according to what you bought previously and what are the tendencies that other people bought when they selected the same product you choose and this new recommendations come because Machine Learning is implemented in many companies to save time to customers and also give offers that the customer could be interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90ffb0",
   "metadata": {},
   "source": [
    "## 1.2 Comparisson between Content and Collaborative Filtering using Books Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65604e",
   "metadata": {},
   "source": [
    "For the delopment of this CA I choose a books dataset that is divided in 3 different excels since it gives different information about the books and the users. Accoding to each question the variables will be selected for the analysis.\n",
    "\n",
    "Before comparing them, let's get familiar with the data exploring inside it and cleaning the data to perform the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a2d042",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrequent_patterns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fpgrowth, association_rules\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrequent_patterns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apriori\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ast import literal_eval\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset\n",
    "df_book = pd.read_csv('Books.csv')\n",
    "df_rating = pd.read_csv('Ratings.csv')\n",
    "df_users = pd.read_csv('Users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe129304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the first 5 rows of df_book\n",
    "df_book.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the first 5 rows of df_rating\n",
    "df_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ce7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the first 5 rows of df_users\n",
    "df_users = df_users.rename(columns=lambda x: x.replace('-','_'))\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a82978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of df_books is', df_book.shape)\n",
    "print('The shape of df_rating is', df_rating.shape)\n",
    "print('The shape of df_users is', df_users.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4f841",
   "metadata": {},
   "source": [
    "After looking the different excel files from the same dataset, we can see that they are related to each other since we can see that df_ratis contains ISBN that is a code repeated in the books datafram with which we can calculate the ratings of the books according to the users. Besides in df_users we can see the location of each user and the age. Due to the different size of each excel file, we can not concatenate; however all the information from each is usefull for the analysis, I will be creating dataframes with the relevant columns for each analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c7ae3",
   "metadata": {},
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Data Dictionary\n",
    "Image(filename =r'Data_Dictionary.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec50b7",
   "metadata": {},
   "source": [
    "### Characterization of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring data types and counting null values\n",
    "df_book.info()\n",
    "df_rating.info()\n",
    "df_users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854ca11",
   "metadata": {},
   "source": [
    "We can see a mixture of numerical discrete, continuous values and objects. However, according to the data dictionary the column \"Year-Of-Publication\" is a numerical value and it is shown as an object. This will be analyzed indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9af24",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb778f",
   "metadata": {},
   "source": [
    "First, let's explore why the column \"Year of Publication\" is detected as an object instead of an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85437b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book['Year-Of-Publication'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed52fef",
   "metadata": {},
   "source": [
    "We can see that instead of a year we have 'DK Publishing Inc', and 'Gallimard' in the columsn and that is the reason it is detected as object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab27321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing \"Year-Of-Publication\" columns\n",
    "df_book[df_book['Year-Of-Publication']=='DK Publishing Inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing \"Year-Of-Publication\" columns\n",
    "df_book[df_book['Year-Of-Publication']=='Gallimard']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3936a3",
   "metadata": {},
   "source": [
    "After looking carefully this 3 colums doesn't have the book author name and since they are represented by object instead of numeric value I am going to drop those columns and convert the column into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns with missing information and transforming the column into numeric\n",
    "df_book = df_book[df_book['Year-Of-Publication'] !='DK Publishing Inc']\n",
    "df_book = df_book[df_book['Year-Of-Publication'] !='Gallimard']\n",
    "df_book['Year-Of-Publication'] = pd.to_numeric(df_book['Year-Of-Publication'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the new shaoe after dropping columns\n",
    "print('The shape of df_books is', df_book.shape)\n",
    "print('The shape of df_rating is', df_rating.shape)\n",
    "print('The shape of df_users is', df_users.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "df_book.drop_duplicates()\n",
    "df_rating.drop_duplicates()\n",
    "df_users.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of df_books is', df_book.shape)\n",
    "print('The shape of df_rating is', df_rating.shape)\n",
    "print('The shape of df_users is', df_users.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112944e6",
   "metadata": {},
   "source": [
    "The shape of the data didn't change after dropping duplicates in all the dataset which means we don't have duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf84654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarizing \"NaN\" values\n",
    "missing_val = [\"n.a.\", \"?\", \"NA\", \"n/a\", \"na\", \"--\", \" \"]\n",
    "df_book = df_book.replace(missing_val, pd.NA)\n",
    "df_rating = df_rating.replace(missing_val, pd.NA)\n",
    "df_users = df_users.replace(missing_val, pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74406399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the percentage of NaN values in each column\n",
    "NaN_book = (df_book.isnull().sum()/len(df_book))*100\n",
    "NaN_rating = (df_rating.isnull().sum()/len(df_rating))*100\n",
    "NaN_users = (df_users.isnull().sum()/len(df_users))*100\n",
    "# NaN values in book dataframe\n",
    "NaN_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b513c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN values in rating dataframe\n",
    "NaN_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff67ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN values in users dataframe\n",
    "NaN_users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4cdbc",
   "metadata": {},
   "source": [
    "the column \"Book-Author\" and \"Publisher\" has less than 1% null values and they are going to be replaced by \"Unknown\", and the column \"Age\" has more than 39% of Nan Values in which if we replace them, we could introduce variance into our data and because of that I am going to drop the complete column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8066a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping \"Age\" column\n",
    "df_users.drop('Age', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ce2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing null values for \"Unknown\"\n",
    "df_book['Book-Author'].fillna('Unknown', inplace= True)\n",
    "df_book['Publisher'].fillna('Unknown', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9cdebf",
   "metadata": {},
   "source": [
    "## Content Based Recommended System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb49533",
   "metadata": {},
   "source": [
    "In content Based recommended Systems we give suggestions to the customers according to the characteristics of the items in this case we have publisher, year of publication, rating and Book-Author as variables that can help us suggest the customers new books to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377d534",
   "metadata": {},
   "source": [
    "### a) Simple Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Book-Rating', data = df_rating)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Ratings plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3fa5a",
   "metadata": {},
   "source": [
    "We have many ratings as zero values that could cause problems with this model. Since this model considers a lot the ratings of the books and gives a recommendation accoring to the number of people that rated the books and the ratings, I will just consider the books that were rated for this analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf86deff",
   "metadata": {},
   "source": [
    "Besides, in the simple recommender system, we need a dataframe in which we can see the vote counts and the vote average of the books. So I will create a data frame with the desired columns and the columns needed for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_no_zeros = df_rating[df_rating['Book-Rating']>0]\n",
    "average_rating = ratings_no_zeros.groupby('ISBN')['Book-Rating'].agg(['mean', 'count']).reset_index()\n",
    "average_rating['mean'] = average_rating['mean'].round(1)\n",
    "average_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b983bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple_BRS = pd.merge(df_book, average_rating, on='ISBN')\n",
    "Simple_BRS.rename(columns={'mean':'Average-Rating', 'count':'Rating-Count'}, inplace = True)\n",
    "Simple_BRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c3ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_Simple_BRS = (Simple_BRS.isnull().sum()/len(Simple_BRS))*100\n",
    "NaN_Simple_BRS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d8b90",
   "metadata": {},
   "source": [
    "I am going to calculate the weighted Rating and define the variables to calculate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e484d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean of rating across the dataframe\n",
    "C = Simple_BRS['Average-Rating'].mean()\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85af87",
   "metadata": {},
   "source": [
    "The average rating (C) is 7.53 on a scale of 10\n",
    "and now let's see the number of books that had scores in the 99th percentile since according to the graphic is where we start having more interactivity in the ratings in I selected a high quantile since I want results in books that had large number of ratings to have more trustable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a832374",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(Simple_BRS['Rating-Count'], bins=70,kde=False, color='skyblue')\n",
    "plt.xlabel('Rating Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of the Rating-Count')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294198ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the minimum number of ratings (m)\n",
    "m = Simple_BRS['Rating-Count'].quantile(0.99)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a491e",
   "metadata": {},
   "source": [
    "The value of m is small since few people rated many books and that is why the distribution gave us m as a small result. If the results of this model show \"Rating-Count\" as low values, m should be changed because we can not trust in final results with few people that rated the books, but if they are considered high, it is good to accept those values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54117810",
   "metadata": {},
   "source": [
    "now, let's filter the movides selecting just the ones that are equal or greater than the minimum number of ratings (m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b65220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the movies\n",
    "q_books = Simple_BRS.copy().loc[Simple_BRS['Rating-Count'] >= m]\n",
    "q_books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the function weighted_rating\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['Rating-Count']\n",
    "    R = x['Average-Rating']\n",
    "    # Calculation based on the IMDB formula\n",
    "    return (v / (v + m) * R) + (m / (m + v) * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa215439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating score vareiable based on the weighted_rating\n",
    "q_books['score'] = q_books.apply(weighted_rating, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the best 15 books according to the score weighted_rating\n",
    "q_books = q_books.sort_values('score', ascending = False)\n",
    "q_books[['Book-Title', 'Book-Author', 'Rating-Count', 'Average-Rating', 'score']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1eeea",
   "metadata": {},
   "source": [
    "The score calculated with wighted_rating function is the one in which we can trust because it considers how many users voted in the movie, the minimum requires, the average rating and the mean across the whole data which gives us a result in which we can trust. In the rating count we can see that many people rated those books which confirms as well that we can trus in m value calculated before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac325b5d",
   "metadata": {},
   "source": [
    "### b) Content Based Recommender analyzing Book-Author, Year-Of-Publication and Publisher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a6928",
   "metadata": {},
   "source": [
    "For this analysis, we are going to focus in the text analysis instead of ratings and we will focus on Book-Title, Book-Author, Year-Of Publication and Publisher, all this rows were in df_books, I am going to create a new dataframe with the information required for this anaysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ContentBR = df_book[['Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher']]\n",
    "df_ContentBR = df_ContentBR.rename(columns=lambda x: x.replace('-','_'))\n",
    "df_ContentBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all words to lower case and dealing with spaces\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    else:\n",
    "        # Check if feature exists. If not, return empty string\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00641ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying clean_data function in the columns\n",
    "sel_features = ['Book_Author', 'Year_Of_Publication', 'Publisher']\n",
    "for feature in sel_features:\n",
    "    df_ContentBR[feature] = df_ContentBR[feature].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f716879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a soup with the words to vectorize them and adding as a column to the dataframe\n",
    "df_ContentBR['mixed_soup'] = df_ContentBR['Book_Author'] + ' ' + df_ContentBR['Year_Of_Publication'].astype(str) + ' ' + df_ContentBR['Publisher']\n",
    "df_ContentBR[['mixed_soup']].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daade57",
   "metadata": {},
   "source": [
    "In this part I decided to analize with different features to have more precision in the modelling since with more information of the books I can predict better the similarities to recommend other books and now I am going to use Count Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Count vectorizer and creating a matrix\n",
    "CV = CountVectorizer(stop_words='english')\n",
    "CV_mat = CV.fit_transform(df_ContentBR['mixed_soup'])\n",
    "CV_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bdac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying cosine_similarity\n",
    "cosine_sim = cosine_similarity(CV_mat[:10000], CV_mat[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50873ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index of your main DataFrame and construct reverse mapping as before\n",
    "df_ContentBR = df_ContentBR.reset_index()\n",
    "indices = pd.Series(df_ContentBR.index, index=df_ContentBR['Book_Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d822860",
   "metadata": {},
   "source": [
    "Since our matrix is big, we are going to apply the model just to the first 12000 rows of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e8bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying  function get_recommendations of the 10 most similar movies\n",
    "def get_recommendations(Book_Title, cosine_sim=cosine_sim):\n",
    "    idx = indices[Book_Title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    return df_ContentBR['Book_Title'].iloc[book_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba617591",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('PLEADING GUILTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea050388",
   "metadata": {},
   "source": [
    "### Colaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c3630",
   "metadata": {},
   "source": [
    "In collaborative filtering we suggest or give advise to the customer according to preferences or similar items that other users have adquired and in this case as we are talking about books it would be recommend a book according to other books or other users with similar characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5af2b",
   "metadata": {},
   "source": [
    "I have a big data from users that rated movies, and that could cause error in our next functions due to the shape of the data. So I will filter and just use the movies that were rated for 50 users or more and ratings marked as zeros will not be considered since they represent users that read the book but didn't give a rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15675bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = df_rating['ISBN'].value_counts()[df_rating['ISBN'].value_counts()>50].index\n",
    "Colab_F = df_rating[df_rating['ISBN'].isin(filters)]\n",
    "Colab_F = Colab_F.rename(columns=lambda x: x.replace('-','_'))\n",
    "Colab_F = Colab_F[Colab_F['Book_Rating'] !=0]\n",
    "Colab_F = Colab_F.reset_index(drop=True)\n",
    "Colab_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e69c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining variables\n",
    "X = Colab_F.copy()\n",
    "y = Colab_F['User_ID']\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c74965",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10884b7c",
   "metadata": {},
   "source": [
    "Before splitting we need to make balance the classes in the dataset since when we count th y values we need to have at least 2 to make correctly the stratification and that is why I am going to filter the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.groupby(y).filter(lambda x: len(x) > 1)\n",
    "X = X[X['User_ID'].isin(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19727767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y ,test_size = 0.2, random_state=42)\n",
    "\n",
    "X.shape, y.shape, X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfe688",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Mean Squared Error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e21193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining baseline 5\n",
    "def baseline(User_ID, ISBN):\n",
    "    return 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Root Mean Squared Error\n",
    "def score(cf_model):\n",
    "    id_pairs = zip(X_test['User_ID'], X_test['ISBN'])\n",
    "    y_pred = np.array([cf_model(User_ID, ISBN) for (User_ID, ISBN) in id_pairs])\n",
    "    y_true = np.array(X_test['Book_Rating'])\n",
    "    return rmse(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ea110",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f9bdb",
   "metadata": {},
   "source": [
    "That is the score baseline that will help us determine if the next models are performing better. \n",
    "\n",
    "I will apply User Based Collaborative Filtering for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creting a rating matrix\n",
    "matrix = X_train.pivot(values = 'Book_Rating', index = 'User_ID', columns = 'ISBN')\n",
    "matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Rating Filtering\n",
    "def cf_user_mean(User_ID, ISBN):\n",
    "    if ISBN in matrix:\n",
    "        mean_rating = matrix[ISBN].mean()\n",
    "    else:\n",
    "        mean_rating = 5.0\n",
    "    return mean_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcad55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute RMSE for the Mean model\n",
    "score(cf_user_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cacb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Weighted Mean with dummie variables\n",
    "dummy_matrix = matrix.copy().fillna(0)\n",
    "dummy_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating cosine similarity into the matrix\n",
    "cosine_sim = cosine_similarity(dummy_matrix, dummy_matrix)\n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd264a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = pd.DataFrame(cosine_sim, index=matrix.index, columns=matrix.index)\n",
    "cosine_sim.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_user_wmean(User_ID, ISBN):\n",
    "    if ISBN in matrix.columns:\n",
    "        sim_scores = cosine_sim[User_ID]\n",
    "        m_ratings = matrix[ISBN]\n",
    "        idx = m_ratings[m_ratings.isnull()].index\n",
    "        m_ratings = m_ratings.dropna()\n",
    "        sim_scores = sim_scores.drop(idx)\n",
    "        \n",
    "        # Replace NaN values with 0.0\n",
    "        m_ratings = m_ratings.fillna(0.0)\n",
    "        #we are calculating the score mean rating, the\n",
    "        wmean_rating = np.dot(sim_scores, m_ratings) / (sim_scores.sum() + 1e-6)  # Adding a small epsilon to avoid division by zero\n",
    "        \n",
    "    else:\n",
    "        wmean_rating = 5.0\n",
    "    \n",
    "    return wmean_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ecba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(cf_user_wmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f22fa",
   "metadata": {},
   "source": [
    "According to the mean score the best model is the Mean Rating filtering model, and in second place comes the weighted mean model; both of them are better than the base line score gotten. Although Mean Rating Filtering shows a better result, I trust and decide to use in this case the weighted Mean Score since due to the use of cosine similarity we can compare between users according to the preferences of other readers and captures the reader behaviour. As the weighted model is lower than the baseline Root Squeared Error metric, this is the one I trust more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14766f",
   "metadata": {},
   "source": [
    "### Add a little bit more of detail in the next question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b491d",
   "metadata": {},
   "source": [
    "Answering the previous question, until this part I presented a clear example of the diferences between content and collaborative filtering in which in content we analyzed what was inside the item for example the publisher, the author and the year and we recommend according to the information we have about the item that in this case is books and now I performed collaborative filtering that can be based on the user preferences like the ratings in the books and based on other ratings from other people that read the books I can recommend other books that people read training the recommendation based on other preferences that users like. Also, collaborative filtering can be used with item to item and generate recommedations with that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93976f",
   "metadata": {},
   "source": [
    "## Train and test machine learning models for item-item collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9faa7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the first 5 rows of the dataframe dataframe\n",
    "Colab_F =pd.merge(Colab_F[['User_ID', 'ISBN', 'Book_Rating']], df_book[['ISBN', 'Book-Title']], on ='ISBN', how='left')\n",
    "Colab_F = Colab_F.rename(columns=lambda x: x.replace('-','_'))\n",
    "Colab_F.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "Colab_F.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming a matrix for rating\n",
    "n_user = Colab_F['User_ID'].nunique()\n",
    "n_books = Colab_F['ISBN'].nunique()\n",
    "isbn_to_index = {isbn: idx for idx, isbn in enumerate(Colab_F['ISBN'].unique())}\n",
    "user_to_index = {user_id: idx for idx, user_id in enumerate(Colab_F['User_ID'].unique())}\n",
    "A = np.zeros((n_user, n_books))\n",
    "for line in Colab_F.itertuples():\n",
    "    user_idx = user_to_index[line[1]]\n",
    "    isbn = line[2]\n",
    "    book_idx = isbn_to_index[isbn]\n",
    "    A[user_idx, book_idx] = line[3]\n",
    "print(\"Original matrix for rating: \", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(A)):\n",
    "    for j in range(len(A[0])):\n",
    "        if A[i][j]>=3:\n",
    "            A[i][j]=1\n",
    "        else:\n",
    "            A[i][j]=0\n",
    "csr_sample = csr_matrix(A)\n",
    "print(csr_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880e692",
   "metadata": {},
   "source": [
    "train_size = 0.8\n",
    "train, test = train_test_split(csr_sample, train_size= train_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5358e5",
   "metadata": {},
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9bafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=3, n_jobs=-1)\n",
    "knn.fit(csr_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e604d966",
   "metadata": {},
   "source": [
    "#Creating the function to predict\n",
    "predictions = []\n",
    "for i in range (test.shape[0]):\n",
    "    _, indices= knn.kneighbors(test[i], n_neighbors=3)\n",
    "    indices = indices.flatten()\n",
    "    predicted_values = np.mean(train[indices], axis =0)\n",
    "    predictions.append(predicted_values)\n",
    "predictions = np.concatenate(predictions)\n",
    "predictions = predictions[:len(true_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89092e35",
   "metadata": {},
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d224f",
   "metadata": {},
   "source": [
    "true_values = test.data\n",
    "true_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8fa73",
   "metadata": {},
   "source": [
    "true_values = test.data\n",
    "flat_pred = predictions.flatten()\n",
    "nonzer_pred = flat_pred[:len(true_values)].reshape(-1,1)\n",
    "mse = mean_squared_error(true_values, nonzer_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print('RMSE is:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sort_des = Colab_F.sort_values(['User_ID', 'ISBN'], ascending=[True, False])\n",
    "filter1 = dataset_sort_des[dataset_sort_des['User_ID'] == 276747].ISBN\n",
    "filter1 = filter1.tolist()\n",
    "filter1 = filter1[:20]\n",
    "liked_books_info = Colab_F[Colab_F['ISBN'].isin(filter1)][['ISBN','Book_Title']].drop_duplicates()\n",
    "print(\"Books liked by user: \",liked_books_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances1 = []\n",
    "indices1 = []\n",
    "recommended_books = set()\n",
    "for i in filter1:\n",
    "    book_idx = isbn_to_index[i]\n",
    "    item_vector = csr_sample[book_idx].toarray().squeeze()\n",
    "    item_vector = item_vector.reshape(1, -1)\n",
    "    distances, indices = knn.kneighbors(item_vector, n_neighbors=3)\n",
    "    indices = indices.flatten()\n",
    "    indices = indices[1:]\n",
    "    indices1.extend(indices)\n",
    "    recommended_books_info = Colab_F.loc[Colab_F.index.isin(indices), ['ISBN', 'Book_Title']].drop_duplicates()\n",
    "    recommended_books.update(map(tuple, recommended_books_info.values.tolist()))\n",
    "print(\"Items (indice number) to be recommended: \", indices1)\n",
    "print(\"\\nBooks recommended: \")\n",
    "for book_info in recommended_books:\n",
    "    print (book_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a12fd5",
   "metadata": {},
   "source": [
    "### 2) Perform Market Basket Analysis on the chosen dataset by using Apriori and FP growth algorithms. Can you express major divergence between these models? Compare and contrast the machine learning results obtained based on both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3207e30",
   "metadata": {},
   "source": [
    "I will use the same dataset that I used in the Colaborative filtering system item - item, but I will add the country in which the user is located because that will improve the model in the part of understanding cultural differences and preferences they have according to the countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac18cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apri_df=Colab_F.copy()\n",
    "Apri_df =pd.merge(Apri_df, df_users[['User_ID', 'Location']], on ='User_ID', how='left')\n",
    "Apri_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apri_df['Country'] = Apri_df['Location'].str.split(',').str[-1].str.strip()\n",
    "Apri_df.drop('Location', axis=1, inplace=True)\n",
    "Apri_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153bf43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apri_df['Book_Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f715ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apri_df['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bab613",
   "metadata": {},
   "source": [
    "As I created a new column taking data from a previous one, I will check the null values and replace them for unknown; so, they won't interfere with the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarizing \"NaN\" values\n",
    "missing_val = [\"n.a.\", \"?\", \"NA\", \"n/a\", \"na\", \"--\", \" \", \"\"]\n",
    "Apri_df = Apri_df.replace(missing_val, pd.NA)\n",
    "null_val = Apri_df['Country'] = null_val = Apri_df['Country'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc49f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_val = Apri_df['Country'].isnull().sum()\n",
    "null_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a723d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apri_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = (Apri_df[Apri_df['Country'] == \"canada\"]\n",
    "          .groupby(['User_ID', 'Book_Title'])['Book_Rating']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('User_ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basket = (Apri_df.groupby(['User_ID', 'Book_Title'])['Book_Rating']\n",
    "#          .max().unstack().reset_index().fillna(0)\n",
    "#          .set_index('User_ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and declare a method named as 'encode_units()'\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "basket_sets = basket.applymap(encode_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0583a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the frequent item rules for aproiori function\n",
    "frequent_itemsets_apri = apriori(basket_sets, min_support = 0.001, use_colnames = True)\n",
    "\n",
    "print(frequent_itemsets_apri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the reles due to apriori algorithm\n",
    "rules_apri = association_rules(frequent_itemsets_apri, metric = \"lift\", min_threshold = 0)\n",
    "\n",
    "print(rules_apri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_rules = rules_apri[(rules_apri['lift']>=5) & (rules_apri['confidence']>=0.8)]\n",
    "print(result_with_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca710ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basket['Key of Light (Key Trilogy (Paperback))'].sum())\n",
    "print(basket['Key of Knowledge (Key Trilogy (Paperback))'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eafe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "basket_usa = (Apri_df[Apri_df['Country'] == \"usa\"]\n",
    "          .groupby(['User_ID', 'Book_Title'])['Book_Rating']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('User_ID'))\n",
    "basket_sets_usa = basket.applymap(encode_units)\n",
    "frequent_itemsets_usa = apriori(basket_sets_usa, min_support = 0.001, use_colnames = True)\n",
    "# Display the reles due to apriori algorithm\n",
    "rules_apri_usa = association_rules(frequent_itemsets_usa, metric = \"lift\", min_threshold = 0)\n",
    "result_with_rules = rules_apri_usa[(rules_apri_usa['lift']>=5) & (rules_apri_usa['confidence']>=0.8)]\n",
    "print(result_with_rules)\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print('Time for execution of this code', duration, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c291261",
   "metadata": {},
   "source": [
    "###  Applying FP-Growth algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493eb7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FP_df = Apri_df.copy()\n",
    "FP_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cdcc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2cc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = (FP_df[FP_df['Country'] == \"canada\"]\n",
    "          .groupby(['User_ID', 'Book_Title'])['Book_Rating']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('User_ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c052b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and declare a method named as 'encode_units()'\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "basket_sets = basket.applymap(encode_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab804c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the frequent item rules for fpgrowth function\n",
    "frequent_itemsets_fp1 = fpgrowth(basket_sets, min_support = 0.001, use_colnames = True)\n",
    "\n",
    "print(frequent_itemsets_fp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa5becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the reles due to fp-growth algorithm\n",
    "rules_fp1 = association_rules(frequent_itemsets_fp1, metric = \"confidence\", min_threshold = 0)\n",
    "\n",
    "print(rules_fp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_rules = rules_fp1[(rules_fp1['lift']>=5) & (rules_fp1['confidence']>=0.8)]\n",
    "print(result_with_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basket['Key of Light (Key Trilogy (Paperback))'].sum())\n",
    "print(basket['Key of Knowledge (Key Trilogy (Paperback))'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "basket_usa = (FP_df[FP_df['Country'] == \"usa\"]\n",
    "          .groupby(['User_ID', 'Book_Title'])['Book_Rating']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('User_ID'))\n",
    "basket_sets_usa = basket.applymap(encode_units)\n",
    "frequent_itemsets_usa = fpgrowth(basket_sets_usa, min_support = 0.001, use_colnames = True)\n",
    "# Display the reles due to apriori algorithm\n",
    "rules_fp_usa = association_rules(frequent_itemsets_usa, metric = \"lift\", min_threshold = 0)\n",
    "result_with_rules = rules_fp_usa[(rules_fp_usa['lift']>=5) & (rules_fp_usa['confidence']>=0.8)]\n",
    "print(result_with_rules)\n",
    "finish = time.time()\n",
    "duration = finish-start\n",
    "print('Time for execution of this code', duration, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f801e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77aa59e3",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a6e82",
   "metadata": {},
   "source": [
    "### 3) Create an interactive Dashboard aimed at older adults (65+) with specific features to summarise the most important aspects of the data and identify through your visualisation why this dataset is suitable for Machine Learning models in an online retail business. Explain how your dashboard is designed with this demographic in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4950571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f31a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
